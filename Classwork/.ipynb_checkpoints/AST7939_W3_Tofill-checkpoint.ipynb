{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 7939 Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter optimization, cross validation, evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the iris data and run cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "iris = load_iris()\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "\n",
    "scores = cross_val_score(model, iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Mean cross-validation scores: {:.3f}\".format(scores.mean()))\n",
    "print(\"Standard deviation cross-validation scores: {:.3f}\".format(scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# If you want to reproduce the result, make sure you use the same random_state value.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use for loops for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_depth = np.arange(10)+1\n",
    "criterion = ['gini','entropy']\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for i in max_depth:\n",
    "    for j in criterion:\n",
    "        model = DecisionTreeClassifier(max_depth=i, criterion=j, random_state=0)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        score = np.mean(score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'max_depth': i, 'criterion': j}\n",
    "    \n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we can take advantage of built-in modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid of parameters in a dictionary \n",
    "param_grid = {'max_depth': np.arange(10)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5, \n",
    "                           return_train_score=True, verbose=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mglearn\n",
    "# If you don't have mglearn install, pip install mglearn.\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "scores = np.array(results.mean_test_score).reshape(2,10)\n",
    "\n",
    "mglearn.tools.heatmap(scores, xlabel='max_depth', xticklabels=param_grid['max_depth'],\n",
    "                     ylabel='criterion', yticklabels=param_grid['criterion'], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we need feature scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We use a \"pipeline\"\n",
    "# Sequentially apply a list of transforms and a final estimator. \n",
    "# Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. \n",
    "# The final estimator only needs to implement fit.\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Parameters of pipelines can be set using '__' separated parameter names:\n",
    "param_grid = {'knn__n_neighbors': np.arange(10)+1,\n",
    "              'knn__weights': ['uniform','distance']}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True, verbose=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the SDSS data we used for homework #1 and test parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('SDSS.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X for an array containing features (i.e., colors)\n",
    "X = np.array([data['u'], data['g'], data['r'], data['i'], data['z']]).T\n",
    "\n",
    "# y for an array containing labels (i.e., galaxies or quasars)\n",
    "y = np.expand_dims(data['class'], axis=1)\n",
    "\n",
    "#If you want to reproduce the result, make sure you use the same random_state value.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization with 1 CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "param_grid = {'max_depth': np.arange(10)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "# By defaults, sklearn's GridSearchCV will use stratified k-fold for classification problems.\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "print(\"Elapsed time: {:.3f}\".format(time.time()-tstart) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the same task with 2 CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "param_grid = {'max_depth': np.arange(10)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=3, n_jobs=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "print(\"Elapsed time: {:.3f}\".format(time.time()-tstart) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data and evaluation metric\n",
    "\n",
    "For all available sklearn's evaluation meterc, see https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate imbalanced dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=2)\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label), alpha=0.5, marker='.')\n",
    "\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': np.arange(10)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5, \n",
    "                           return_train_score=True, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got 97% accuracy! But let's have a look at the result (make this as a habit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y_test)\n",
    "print(counter)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y_test == label)[0]\n",
    "    plt.scatter(X_test[row_ix, 0], X_test[row_ix, 1], label=str(label), alpha=1, marker='.')\n",
    "\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is our best model. Are we good with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** operator takes a dictionary and unpack it into keaword arguments in a function.\n",
    "model = DecisionTreeClassifier(**grid_search.best_params_, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "counter = Counter(prediction)\n",
    "print(counter)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(prediction == label)[0]\n",
    "    plt.scatter(X_test[row_ix, 0], X_test[row_ix, 1], label=str(label), alpha=1, marker='.')\n",
    "\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although we got a pretty high accuracy, this may not necesarily the result we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out different evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: I'd like to minimize false negatives. Let's use \"recall\" as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make a plot showing your prediction. Is it any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "angle = np.pi/5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(m, 2)/10\n",
    "X = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X_scaled[:, 0], X_scaled[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(X_scaled.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the two features are highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X_pca[:, 0], X_pca[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.xlabel(\"$PC_1$\", fontsize=18)\n",
    "plt.ylabel(\"$PC_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X_scaled[:, 0], X_scaled[:, 1], \"bo\", alpha=0.5)\n",
    "\n",
    "# Plotting PC1\n",
    "plt.arrow(0,0,pca.components_[0,0],pca.components_[0,1], width=0.1, color='r', zorder=10)\n",
    "\n",
    "# Plotting PC2\n",
    "plt.arrow(0,0,pca.components_[1,0],pca.components_[1,1], width=0.1, color='g', zorder=10)\n",
    "\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance along the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the covariance matrix after PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(X_pca.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two PCs are uncorelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance ratio along the PCs (i.e., importance of the \"new\" features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(X_pca.T)[0,0], np.cov(X_pca.T)[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X_pca[:, 0], np.zeros(X_pca.shape), \"bo\", alpha=0.5)\n",
    "\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.xlabel(\"$PC_1$\", fontsize=18)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_inverse = pca.inverse_transform(X_pca)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(X_inverse[:, 0], X_inverse[:, 1], \"bo\", alpha=0.5)\n",
    "\n",
    "plt.axis([-4, 4, -4, 4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction is effective in removing highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with the breast cancer dataset.\n",
    "This example is adopted from: https://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a DT model and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = pd.Series(cancer.target)\n",
    "\n",
    "# Make train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(model.score(X_test, y_test)))\n",
    "\n",
    "# Make the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how many components we want in PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_scaled)\n",
    "\n",
    "# Get explained variances\n",
    "print(\"Variances (Percentage):\")\n",
    "print(model.explained_variance_ratio_ * 100)\n",
    "print()\n",
    "\n",
    "print(\"Cumulative Variance (Percentage):\")\n",
    "print(np.cumsum(model.explained_variance_ratio_ * 100))\n",
    "print()\n",
    "\n",
    "# Make the scree plot\n",
    "plt.plot(np.cumsum(model.explained_variance_ratio_ * 100))\n",
    "plt.xlabel(\"Number of components (Dimensions)\")\n",
    "plt.ylabel(\"Cumulative variance (%)\")\n",
    "\n",
    "plt.savefig('explained_variance.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=7)\n",
    "\n",
    "# This is the data in the \"new\" low-dimensional feature space.\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca = pd.DataFrame(X_pca)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check correlation coefficients of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_pca.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to do this is to determine how much variance you'd like to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA, 90% variance\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "# This is the data in the \"new\" low-dimensional feature space.\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca = pd.DataFrame(X_pca)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractional variance and the number of components to achieve that fractional variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components, pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a DT model using the \"PCA-ed\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca) # Predictions\n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "print(\"Test score: {:.2f}\".format(model.score(X_test_pca, y_test)))\n",
    "\n",
    "# Make the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nTest confusion_matrix\")\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same accuracy of 0.9 in the lower dimensional space (7 vs. 30), which is great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's move to even higher-dimesional space.  We will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X = X[::10]\n",
    "y = y[::10]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make a kNN model using k=10 without PCA. Print out test score so that we can compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Now apply PCA before we make a kNN model while keeping 90% variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How many components did you end up keeping and how does that compare with the original dimension (i.e., 784)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Compare some of the original images and their inverse transformed images from the data that contain only 90% of the original information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's plot some of the Eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8));\n",
    "\n",
    "plt.subplot(2, 5, 1)\n",
    "plt.imshow(pca.components_[0].reshape(28,28))\n",
    "plt.title('PC1', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 2)\n",
    "plt.imshow(pca.components_[1].reshape(28,28))\n",
    "plt.title('PC2', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 3)\n",
    "plt.imshow(pca.components_[2].reshape(28,28))\n",
    "plt.title('PC3', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 4)\n",
    "plt.imshow(pca.components_[3].reshape(28,28))\n",
    "plt.title('PC4', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 5)\n",
    "plt.imshow(pca.components_[4].reshape(28,28))\n",
    "plt.title('PC5', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 6)\n",
    "plt.imshow(pca.components_[9].reshape(28,28))\n",
    "plt.title('PC10', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 7)\n",
    "plt.imshow(pca.components_[19].reshape(28,28))\n",
    "plt.title('PC20', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 8)\n",
    "plt.imshow(pca.components_[29].reshape(28,28))\n",
    "plt.title('PC30', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 9)\n",
    "plt.imshow(pca.components_[39].reshape(28,28))\n",
    "plt.title('PC40', fontsize = 14);\n",
    "\n",
    "plt.subplot(2, 5, 10)\n",
    "plt.imshow(pca.components_[49].reshape(28,28))\n",
    "plt.title('PC50', fontsize = 14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What does the figure tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Galaxy Zoo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install skimage, see https://scikit-image.org/docs/stable/install.html\n",
    "\n",
    "import glob\n",
    "import skimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "path = './galaxyzoo/'\n",
    "\n",
    "images = []\n",
    "\n",
    "fig, axes = plt.subplots(ncols= 5, nrows = 5, figsize=(50,50))\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i, file in enumerate(glob.glob(path+\"*\")):\n",
    "    img = skimage.io.imread(file)\n",
    "    img = img.mean(axis=2)\n",
    "    if i < 25:\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "    img_resized = resize(img,(100,100))\n",
    "    length = np.prod(img_resized.shape)\n",
    "    img_resized = np.reshape(img_resized,length)\n",
    "    images.append(img_resized)\n",
    "    \n",
    "images = np.vstack(images)\n",
    "\n",
    "images /= 255 # normalize such that each pixel is in between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Compute mean squared error when you keep 80%, 90%, and 95% of the variance. How does the error vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Keep 90% of the variance and apply PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Inverse transform the PCA-ed images. Make figures showing (1) original image, (2) inverse transformed image, and (3) the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

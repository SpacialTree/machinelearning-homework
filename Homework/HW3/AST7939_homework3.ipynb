{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 7939 Homework Assignment #3 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savannah Gramze\n",
    "\n",
    "Use photometric data with different filters to infer the redshift.\n",
    "\n",
    "If we have photometry and spectroscopy for certain galaxies, we can use machine learning to train to infer the redshifts of certain galaxies using photometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All work is due Monday March 6 at 5 pm.\n",
    "\n",
    "## Instruction: \n",
    "Do all homework in this Jupyter notebook and submit your final .ipynb file via Canvas. Show ALL your work and try to add comment lines as needed to describe what your code does. \n",
    "\n",
    "You are encouraged to discuss homework problems with your classmates. However, your python script and answers to the questions must be written by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Photometric redshift estimator using Random Forest (7 pts)\n",
    "\n",
    "Following [Zhou et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019MNRAS.488.4565Z/abstract), the goal of this homework is to estimate photometric redshifts starting from observations of galaxy magnitudes in six different photometric bands (u, g, r, i, z, y), and compare the estimated photometric redshifts with spectroscopic redshifts. The data used in the paper are available at this [link](http://d-scholarship.pitt.edu/36064/). This example is motivated by an example in Machine Learning techniques for Physics and Astronomy that is being written by Dr. Viviana Acquaviva.\n",
    "\n",
    "Redshift is a crucial observable in the study of galaxies and cosmology. Spectroscopic redshifts are accurate, but the observations required are much more expensive than photometric measurements. Modern imaging surveys can measure the photometry of a huge number of objects very efficiently, but only a very small fraction will have observed spectra. For such surveys, redshifts must be estimated from broad-band photometry, and the large number of photometric redshift measurements compensates for their inaccuracy. The availability of large imaging data sets has made photometric redshift estimates an increasingly important component of modern extragalactic astronomy and cosmology studies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper used ugriz photometry from Canada–France–Hawaii Telescope Legacy Survey (CFHTLS) and Y-band photometry from the Subaru Suprime camera, as well as spectroscopic redshifts from the DEEP2, DEEP3, and 3D-HST surveys. In this homework problem, let's use DEEP2/3 data stored in DEEP2_uniq_Terapix_Subaru_v1.fits. You can read in the fits file by executing the cell below. See README file for more information on the content of the fits file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with fits.open('./DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) As in almost all real-life ML problems, start with data cleaning. Follow the steps below. (1 pt)\n",
    "\n",
    "1. Collect deep field CFHTLS data only by choosing the objects with `cfhtls_source` = 0.\n",
    "2. Collect deep source subaru data only by choosing the objects with `subaru_source` = 0.\n",
    "3. Collect objects with redshift quality code `zquality` >= 3.\n",
    "4. Collect objects with `u_apercor` < 99.\n",
    "5. Collect objects with `y_apercor` < 99.\n",
    "\n",
    "The 'features' we will use are aperture corrected ugrizy magnitudes, `u_apercor, g_apercor, r_apercor, i_apercor, z_apercor, y_apercor`. The 'label' we will use is the spectroscopic redshift `zhelio`. Put the features in a dataframe X and labels in a dataframe y. You should end up with 5874 objects after data cleaning. Split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdtls_data = df[df['cfhtls_source'] == 0]#.keys()\n",
    "deep_source = cfdtls_data[cfdtls_data['subaru_source'] == 0]\n",
    "quality_red = deep_source[deep_source['zquality'] >= 3.]\n",
    "clean_u = quality_red[quality_red['u_apercor'] < 99]\n",
    "clean_df = clean_u[clean_u['y_apercor'] < 99]\n",
    "\n",
    "X = clean_df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor', 'y_apercor']]\n",
    "y = clean_df['zhelio']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Make a `RandomForestRegressor` model. Use the default hyperparameters for now (we will optimize them later). Compute the accuracy of the model in two ways. (1 pt)\n",
    "\n",
    "1. Compute the training and test scores of the model using 5-fold cross validation. You will get 5 scores. Report the average of the 5 scores.\n",
    "2. Compute the out-of-bag test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# every time you bootstrap, there are some data left in original dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "#RandomForestClassifier?\n",
    "#LabelEncoder?\n",
    "#KFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Using the `RandomForestRegressor` you built in #b, make predictions for the test dataset. This prediction will be the \"photometric redshift\". Then, make a scatter plot showing photometric redshift vs. spectroscopic redshift. Your figure should look similar to Figure 5 of Zhou et al. (2019). Also, compute the outlier fraction $\\eta$, which is defined as the fraction of objects with $|z_{phot}-z_{spec}| > 0.15/(1+z_{spec})$. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test dataset\n",
    "# make a plot showing inferred photometric redshift vs actual spectroscopic redshift\n",
    "# 6% of the data are outliers in the paper\n",
    "# How much of the data are outliers?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Let's try to optimize hyperparameters `max_features` and `n_estimators`. For `max_features`, try three different values: None, \"sqrt\", and 2. Compute the test scores of the best model and compare them with what you got in #b. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize max_features in random forest\n",
    "# try None, sqrt and 2 for max_features (# of dimensions?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) You might have found that the test scores are not drastically improved. What should we do? Let's try to add u-g, g-r, r-i, i-z, z-y colors and see if it improves the performance. You now have a 11-dimensional dataset. Using this new dataset, make a Random Forest, optimize hyperparameters, and compute the test score of the best model. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-80%, not super good\n",
    "# to try to improve, add more features, ie colors\n",
    "# make 5 colors and add them for 11 dimensions \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) With your best model, repeat #c. Make a plot showing photometric redshift vs. spectroscopic redshift, and compute the outlier fraction. Do you find better perfomance? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Make a plot showing feature importance. Which is more important between colors and magnitudes? Does the feature importance plot support that adding colors helped improve the performance? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show feature importance bar graph \n",
    "# which are the most important features?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Photometric redshift estimator using Gradient Boosting (3 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's use Gradient Boosting to compute photometric redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Let's use `max_depth=2`. What `n_estimator` should we use? Make a plot showing the validation error vs. n_estimator and find the optimal `n_estimator`. What is your optimal `n_estimator` and what is the test score using the optimal model? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting, gradient\n",
    "# use decision tree with max_depth=2\n",
    "# can use as many n_estimator as you want, but what is optimal?\n",
    "# find the best gradient boosting model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Using the `GradientBoostingRegressor` you built in #a, make predictions for the test dataset. This prediction will be the \"photometric redshift\". Then, make a scatter plot showing photometric redshift vs. spectroscopic redshift. Your figure should look similar to Figure 5 of Zhou et al. (2019). Also, compute the outlier fraction $\\eta$, which is defined as the fraction of objects with $|z_{phot}-z_{spec}| > 0.15/(1+z_{spec})$. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Make a movie showing the evolution of photometric redshift vs. spectroscopic redshift as you increase `n_estimator`. See gbr.mp4 on Canvas as an example. Discuss what you see in the movie in the context of boosting. Submit your movie file. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a movie??????\n",
    "# make a png for each n_estimator and then combine into 1000 frame mp4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You reproduced results presented in yet another journal paper. If you enjoyed the homework and would like to do some additional related analysis, I recommend you have a look at the following papers and references therein. Feel free to read/reproduce them for paper presentation and/or term project.\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2010ApJ...712..511C/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2021MNRAS.505.4847H/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.2289L/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.1696H/abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

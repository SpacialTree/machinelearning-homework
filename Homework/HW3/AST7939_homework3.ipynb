{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 7939 Homework Assignment #3 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Savannah Gramze\n",
    "\n",
    "Use photometric data with different filters to infer the redshift.\n",
    "\n",
    "If we have photometry and spectroscopy for certain galaxies, we can use machine learning to train to infer the redshifts of certain galaxies using photometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All work is due Monday March 6 at 5 pm.\n",
    "\n",
    "## Instruction: \n",
    "Do all homework in this Jupyter notebook and submit your final .ipynb file via Canvas. Show ALL your work and try to add comment lines as needed to describe what your code does. \n",
    "\n",
    "You are encouraged to discuss homework problems with your classmates. However, your python script and answers to the questions must be written by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Photometric redshift estimator using Random Forest (7 pts)\n",
    "\n",
    "Following [Zhou et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019MNRAS.488.4565Z/abstract), the goal of this homework is to estimate photometric redshifts starting from observations of galaxy magnitudes in six different photometric bands (u, g, r, i, z, y), and compare the estimated photometric redshifts with spectroscopic redshifts. The data used in the paper are available at this [link](http://d-scholarship.pitt.edu/36064/). This example is motivated by an example in Machine Learning techniques for Physics and Astronomy that is being written by Dr. Viviana Acquaviva.\n",
    "\n",
    "Redshift is a crucial observable in the study of galaxies and cosmology. Spectroscopic redshifts are accurate, but the observations required are much more expensive than photometric measurements. Modern imaging surveys can measure the photometry of a huge number of objects very efficiently, but only a very small fraction will have observed spectra. For such surveys, redshifts must be estimated from broad-band photometry, and the large number of photometric redshift measurements compensates for their inaccuracy. The availability of large imaging data sets has made photometric redshift estimates an increasingly important component of modern extragalactic astronomy and cosmology studies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper used ugriz photometry from Canada–France–Hawaii Telescope Legacy Survey (CFHTLS) and Y-band photometry from the Subaru Suprime camera, as well as spectroscopic redshifts from the DEEP2, DEEP3, and 3D-HST surveys. In this homework problem, let's use DEEP2/3 data stored in DEEP2_uniq_Terapix_Subaru_v1.fits. You can read in the fits file by executing the cell below. See README file for more information on the content of the fits file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with fits.open('./DEEP2_uniq_Terapix_Subaru_v1.fits') as data:\n",
    "    df = pd.DataFrame(np.array(data[1].data).byteswap().newbyteorder()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) As in almost all real-life ML problems, start with data cleaning. Follow the steps below. (1 pt)\n",
    "\n",
    "1. Collect deep field CFHTLS data only by choosing the objects with `cfhtls_source` = 0.\n",
    "2. Collect deep source subaru data only by choosing the objects with `subaru_source` = 0.\n",
    "3. Collect objects with redshift quality code `zquality` >= 3.\n",
    "4. Collect objects with `u_apercor` < 99.\n",
    "5. Collect objects with `y_apercor` < 99.\n",
    "\n",
    "The 'features' we will use are aperture corrected ugrizy magnitudes, `u_apercor, g_apercor, r_apercor, i_apercor, z_apercor, y_apercor`. The 'label' we will use is the spectroscopic redshift `zhelio`. Put the features in a dataframe X and labels in a dataframe y. You should end up with 5874 objects after data cleaning. Split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdtls_data = df[df['cfhtls_source'] == 0]#.keys()\n",
    "deep_source = cfdtls_data[cfdtls_data['subaru_source'] == 0]\n",
    "quality_red = deep_source[deep_source['zquality'] >= 3.]\n",
    "clean_u = quality_red[quality_red['u_apercor'] < 99]\n",
    "clean_df = clean_u[clean_u['y_apercor'] < 99]\n",
    "\n",
    "X = clean_df[['u_apercor', 'g_apercor', 'r_apercor', 'i_apercor', 'z_apercor', 'y_apercor']]\n",
    "y = clean_df['zhelio']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Make a `RandomForestRegressor` model. Use the default hyperparameters for now (we will optimize them later). Compute the accuracy of the model in two ways. (1 pt)\n",
    "\n",
    "1. Compute the training and test scores of the model using 5-fold cross validation. You will get 5 scores. Report the average of the 5 scores.\n",
    "2. Compute the out-of-bag test score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# every time you bootstrap, there are some data left in original dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "#KFold?\n",
    "#RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7286793325347354"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "model.oob_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [[23.14385455 22.57061078 22.07681547 21.96614953 21.88239458 21.86940264]\n",
      " [22.90808034 21.69142985 20.34757164 19.52116763 19.16546715 19.02539304]\n",
      " [23.92743724 23.83899934 23.65775313 23.37988912 22.83812948 22.67727385]\n",
      " ...\n",
      " [23.00222627 22.83969551 22.24057104 21.93963208 21.99231725 21.84361009]\n",
      " [22.95836299 22.80062735 22.637572   22.10905323 21.9311262  22.28831105]\n",
      " [25.2195741  24.81068257 24.12032687 23.83355862 23.71999111 23.75485062]], test: [[23.42081552 23.17718898 23.09128725 22.88009722 22.7546036  22.30554184]\n",
      " [24.88916427 24.72872719 24.10570779 23.76448347 23.7949746  24.03178548]\n",
      " [24.49023882 24.23167237 23.55026683 23.07315501 22.94451298 22.86465101]\n",
      " ...\n",
      " [23.15444628 21.77713988 20.45626145 19.90422633 19.6271698  19.44926193]\n",
      " [23.57553899 23.58241547 23.42643695 23.20378427 22.8177214  23.61729998]\n",
      " [24.911186   24.47307877 23.6576746  22.98649944 22.30333954 22.03850741]]\n",
      "train: [[23.14385455 22.57061078 22.07681547 21.96614953 21.88239458 21.86940264]\n",
      " [22.90808034 21.69142985 20.34757164 19.52116763 19.16546715 19.02539304]\n",
      " [23.42081552 23.17718898 23.09128725 22.88009722 22.7546036  22.30554184]\n",
      " ...\n",
      " [22.95836299 22.80062735 22.637572   22.10905323 21.9311262  22.28831105]\n",
      " [24.911186   24.47307877 23.6576746  22.98649944 22.30333954 22.03850741]\n",
      " [25.2195741  24.81068257 24.12032687 23.83355862 23.71999111 23.75485062]], test: [[24.77029107 24.50367427 23.90912194 23.13663844 22.67433773 22.75321518]\n",
      " [23.65066524 23.57087144 23.29805596 22.91265192 22.50069049 22.45895377]\n",
      " [24.10805107 23.83865051 23.26829674 22.99728721 23.0363329  22.74289401]\n",
      " ...\n",
      " [23.56117848 23.37286029 22.91664528 22.51506101 22.36338582 22.67068113]\n",
      " [25.25020927 24.67301004 23.90691547 23.68339468 23.52522714 23.5431653 ]\n",
      " [24.07071957 23.7687241  23.10430561 22.48793528 22.31367154 22.21291248]]\n",
      "train: [[22.90808034 21.69142985 20.34757164 19.52116763 19.16546715 19.02539304]\n",
      " [23.42081552 23.17718898 23.09128725 22.88009722 22.7546036  22.30554184]\n",
      " [23.92743724 23.83899934 23.65775313 23.37988912 22.83812948 22.67727385]\n",
      " ...\n",
      " [22.95836299 22.80062735 22.637572   22.10905323 21.9311262  22.28831105]\n",
      " [24.911186   24.47307877 23.6576746  22.98649944 22.30333954 22.03850741]\n",
      " [25.2195741  24.81068257 24.12032687 23.83355862 23.71999111 23.75485062]], test: [[23.14385455 22.57061078 22.07681547 21.96614953 21.88239458 21.86940264]\n",
      " [22.88614367 22.36953976 21.48441882 21.10196115 20.94127633 20.69056838]\n",
      " [24.85242033 24.62320484 24.01283518 23.51139507 22.86556447 22.47277517]\n",
      " ...\n",
      " [23.63025401 23.15954448 22.37107296 21.88321409 21.71328417 21.59220076]\n",
      " [23.21486184 22.23326627 21.45735965 21.03858967 20.84105689 20.76074488]\n",
      " [23.43504077 23.41254728 23.40022712 23.28310339 22.86656261 23.19593659]]\n",
      "train: [[23.14385455 22.57061078 22.07681547 21.96614953 21.88239458 21.86940264]\n",
      " [23.42081552 23.17718898 23.09128725 22.88009722 22.7546036  22.30554184]\n",
      " [24.77029107 24.50367427 23.90912194 23.13663844 22.67433773 22.75321518]\n",
      " ...\n",
      " [22.95836299 22.80062735 22.637572   22.10905323 21.9311262  22.28831105]\n",
      " [24.911186   24.47307877 23.6576746  22.98649944 22.30333954 22.03850741]\n",
      " [25.2195741  24.81068257 24.12032687 23.83355862 23.71999111 23.75485062]], test: [[22.90808034 21.69142985 20.34757164 19.52116763 19.16546715 19.02539304]\n",
      " [23.92743724 23.83899934 23.65775313 23.37988912 22.83812948 22.67727385]\n",
      " [23.04945006 22.2925533  22.03306356 21.88559699 21.87342574 21.89324281]\n",
      " ...\n",
      " [24.91169899 24.71420708 24.06238832 23.46400637 22.8421263  22.63691064]\n",
      " [24.38751537 24.31667619 23.98284011 23.58894666 23.09167562 23.08224849]\n",
      " [23.00222627 22.83969551 22.24057104 21.93963208 21.99231725 21.84361009]]\n",
      "train: [[23.14385455 22.57061078 22.07681547 21.96614953 21.88239458 21.86940264]\n",
      " [22.90808034 21.69142985 20.34757164 19.52116763 19.16546715 19.02539304]\n",
      " [23.42081552 23.17718898 23.09128725 22.88009722 22.7546036  22.30554184]\n",
      " ...\n",
      " [23.00222627 22.83969551 22.24057104 21.93963208 21.99231725 21.84361009]\n",
      " [23.57553899 23.58241547 23.42643695 23.20378427 22.8177214  23.61729998]\n",
      " [24.911186   24.47307877 23.6576746  22.98649944 22.30333954 22.03850741]], test: [[24.04573014 23.90672133 23.65579577 23.04220538 22.69195837 22.49121331]\n",
      " [23.43973848 22.99521143 22.26004199 22.08292504 22.03077623 21.9588282 ]\n",
      " [24.94131391 24.92078799 24.95243    24.45433793 24.38579621 24.54747673]\n",
      " ...\n",
      " [24.40187872 24.07413215 23.18532981 22.3527393  21.63790951 21.54756518]\n",
      " [22.95836299 22.80062735 22.637572   22.10905323 21.9311262  22.28831105]\n",
      " [25.2195741  24.81068257 24.12032687 23.83355862 23.71999111 23.75485062]]\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(X_train):\n",
    "    print('train: %s, test: %s' % (X_train[train], X_train[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Using the `RandomForestRegressor` you built in #b, make predictions for the test dataset. This prediction will be the \"photometric redshift\". Then, make a scatter plot showing photometric redshift vs. spectroscopic redshift. Your figure should look similar to Figure 5 of Zhou et al. (2019). Also, compute the outlier fraction $\\eta$, which is defined as the fraction of objects with $|z_{phot}-z_{spec}| > 0.15/(1+z_{spec})$. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test dataset\n",
    "# make a plot showing inferred photometric redshift vs actual spectroscopic redshift\n",
    "# 6% of the data are outliers in the paper\n",
    "# How much of the data are outliers?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Let's try to optimize hyperparameters `max_features` and `n_estimators`. For `max_features`, try three different values: None, \"sqrt\", and 2. Compute the test scores of the best model and compare them with what you got in #b. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize max_features in random forest\n",
    "# try None, sqrt and 2 for max_features (# of dimensions?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) You might have found that the test scores are not drastically improved. What should we do? Let's try to add u-g, g-r, r-i, i-z, z-y colors and see if it improves the performance. You now have a 11-dimensional dataset. Using this new dataset, make a Random Forest, optimize hyperparameters, and compute the test score of the best model. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-80%, not super good\n",
    "# to try to improve, add more features, ie colors\n",
    "# make 5 colors and add them for 11 dimensions \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) With your best model, repeat #c. Make a plot showing photometric redshift vs. spectroscopic redshift, and compute the outlier fraction. Do you find better perfomance? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Make a plot showing feature importance. Which is more important between colors and magnitudes? Does the feature importance plot support that adding colors helped improve the performance? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show feature importance bar graph \n",
    "# which are the most important features?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Photometric redshift estimator using Gradient Boosting (3 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's use Gradient Boosting to compute photometric redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Let's use `max_depth=2`. What `n_estimator` should we use? Make a plot showing the validation error vs. n_estimator and find the optimal `n_estimator`. What is your optimal `n_estimator` and what is the test score using the optimal model? (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting, gradient\n",
    "# use decision tree with max_depth=2\n",
    "# can use as many n_estimator as you want, but what is optimal?\n",
    "# find the best gradient boosting model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Using the `GradientBoostingRegressor` you built in #a, make predictions for the test dataset. This prediction will be the \"photometric redshift\". Then, make a scatter plot showing photometric redshift vs. spectroscopic redshift. Your figure should look similar to Figure 5 of Zhou et al. (2019). Also, compute the outlier fraction $\\eta$, which is defined as the fraction of objects with $|z_{phot}-z_{spec}| > 0.15/(1+z_{spec})$. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Make a movie showing the evolution of photometric redshift vs. spectroscopic redshift as you increase `n_estimator`. See gbr.mp4 on Canvas as an example. Discuss what you see in the movie in the context of boosting. Submit your movie file. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a movie??????\n",
    "# make a png for each n_estimator and then combine into 1000 frame mp4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You reproduced results presented in yet another journal paper. If you enjoyed the homework and would like to do some additional related analysis, I recommend you have a look at the following papers and references therein. Feel free to read/reproduce them for paper presentation and/or term project.\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2010ApJ...712..511C/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2021MNRAS.505.4847H/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.2289L/abstract\n",
    "\n",
    "https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.1696H/abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
